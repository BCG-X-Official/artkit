{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating New Model Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook serves as a primer for developing ARTKIT model classes. This is most likely your first entrypoint to contributing to ARTKIT. So, to get you started we will cover:\n",
    "\n",
    "- **Object-oriented programming motivations:** The advantages of using class hierarchies and how this enables us to distribute features across client implementations\n",
    "- **ARTKIT's model class hierarchy:** How are classes structured across ARTKIT and what levels of abstraction exist\n",
    "- **Model implementation example:** A deep-dive of the initialization and `get_response` implementation of the `OpenAIChat` class\n",
    "- **Your implementation - a checklist:** Steps to complete to code up your own client implementation\n",
    "\n",
    "This is a technical guide for users who are looking to implement new functionalities within the ARTKIT library. While a basic understanding of python classes is assumed, this guide should be accessible for anyone with a data science background.\n",
    "\n",
    "To get more information on the classes mentioned here go to the [API reference](../../apidoc/artkit.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-oriented programming motivations\n",
    "\n",
    "ARTKIT makes heavy use of class inheritance to implement connectors to GenAI model providers. This allows us to take advantage of features specific to certain model providers while standardizing common utility methods such as asynchronous execution, managing chat history, and caching. It also means you can quickly implement classes to connect to new providers and immediately re-use these standard features without any additional coding.\n",
    "\n",
    "**When should I create a custom model class?** Currently ARTKIT provides interfaces for connecting to OpenAI, Anthropic, Hugging Face, Groq, and Google's Gemini LLMs. It also supports OpenAI's multi-modal models, specifically DALL-E models and vision endpoints. If there are additional model providers you'd like to use in your testing and evaluation pipeline, creating a custom class is the best way to do so -- and also improves the usefulness of ARTKIT for everyone! To get to know more about contributing to ARTKIT see our [Contributor Guide](../../contributor_guide/index.rst).\n",
    "\n",
    "**What if I want to change core class functionality?** Reviewing the model class hierarchy in this guide is a good starting point to developing new core features to the library, though implementing and testing these features will more take careful considerations and scrutiny, as introducing changes on higher levels of abstraction can have implications for other client implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARTKIT's model class hierarchy\n",
    "\n",
    "Here's a summary of the model class hierarchy, starting from the abstract `GenAIModel` to the `OpenAIChat` class:\n",
    "\n",
    "1. `GenAIModel` is an abstract representation of a generic Gen AI model\n",
    "2. `ConnectorMixin` is a mixin class that adds a common interface for client connections to a `GenAIModel`\n",
    "3. `ChatModel` is a an abstract class to have a common interface for generating responses for text prompts, emulating a chat - its a subclass of a `GenAIModel`\n",
    "4. `ChatModelConnector` combines the `ConnectorMixin` with the `ChatModel` interface to create chat model that connects to a client\n",
    "5. `OpenAIChat` is a subclass of `ChatModelConnector` to connect to OpenAI's model and utilize them for chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tree of the relevant repo structure, so you can take a look at the implementation of these classes:\n",
    "\n",
    "```\n",
    "model/\n",
    "  base/\n",
    "    _model.py     --> GenAIModel, ConnectorMixin\n",
    "  llm/\n",
    "    base/\n",
    "      _llm.py     --> ChatModel, ChatModelConnector\n",
    "    openai/\n",
    "      _openai.py  --> OpenAIChat\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar hierarchy is shared for other model providers and modalities. For example, levels 1. - 4. are identical for the `AnthropicChat` class. This means that `OpenAIChat` and `AnthropicChat` share all attributes and method signatures of `ChatModelConnector`, such as `get_client()` but not the actual implementation of those methods. This enables model-specific parameters and response processing. \n",
    "\n",
    "***If you're adding a new custom model class, you will most likely only need to make changes on hierarchy level 5.***\n",
    "\n",
    "It is similar for other modalities; for example, the `OpenAIDiffusion` class inherits from a `DiffusionModelConnector`, which is again a subclass of a `DiffusionModel` and a `ConnectorMixin`. Unlike a `ChatModel`, a `DiffusionModel` does not have a `get_response()` method. However, because it also inherits from `ConnectorMixin`, it will have the same `get_client()` method signature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation example\n",
    "\n",
    "Next, we will demonstrate how the model class hierarchy is used through an example of the `OpenAIChat` class. \n",
    "\n",
    "The goal is to illustrate both (i) what methods you need to implement to create a custom model class and (ii) why those methods are set up the way that they are. \n",
    "\n",
    "A `ChatModelConnector` has two required parameters: `model_id` and `api_env_key`. The `OpenAIChat` has additional model-specific parameters such as `temperature`. So when we initialize an `OpenAIChat` object as below, we're also initializing the superclass to handle client connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ARTKIT\n",
    "import artkit.api as ak\n",
    "\n",
    "# Load API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "gpt4_chat = ak.OpenAIChat(model_id=\"gpt-4\", api_key_env=\"OPENAI_API_KEY\", temperature=0.5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a chat model connector the following methods need to be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "### model/llm/openai/_openai.py   ###\n",
    "#####################################\n",
    "from typing import Any\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from artkit.model.llm.base import ChatModelConnector\n",
    "from artkit.model.llm.history import ChatHistory\n",
    "\n",
    "\n",
    "class OpenAIChat(ChatModelConnector[AsyncOpenAI]):\n",
    "\n",
    "    # required by ConnectorMixin to establish a client connection\n",
    "    @classmethod\n",
    "    def get_default_api_key_env(cls) -> str:\n",
    "        pass\n",
    "       \n",
    "    # required by ConnectorMixin to establish a client connection\n",
    "    def _make_client(self) -> AsyncOpenAI:  \n",
    "        pass\n",
    "\n",
    "    # required by ChatModel to send a message to the model\n",
    "    async def get_response(  # pragma: no cover\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        history: ChatHistory | None = None,\n",
    "        **model_params: dict[str, Any],\n",
    "    ) -> list[str]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By unifying the interfaces for chat models we are enabled to follow a delegation pattern through which we can add external behaviors to all `ChatModelConnectors` by wrapping them in a separate class. `CachedChatModel` is a great example of this that enables us to cache requests to a model provider without caring about the actual implementation details. You can see us make use of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model in a CachedChatModel\n",
    "gpt4_chat = ak.CachedChatModel(\n",
    "    model=gpt4_chat,\n",
    "    database=\"cache/creating_custom_model_classes.db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a deeper dive into the `OpenAIChat` implementation of `get_response()` to highlight how we're taking advantage of the full class hierarchy: \n",
    "\n",
    "1. The `with_system_prompt()` method, that you will see later, is inherited from the `ChatModelConnector` superclass to set the `system_prompt` property of the model\n",
    "2. Upon calling `get_response()`, we first use AsyncExitStack() to enter the model's context manager\n",
    "3. Then we call the `get_client()` method of the `ConnectorMixin` superclass to fetch a cached OpenAI client instance.\n",
    "4. If no previous client instance exists, it is created via the model's `_make_client()` implementation. This logic rests in the `ConnectorMixin` as well.\n",
    "5. Next, we format an input message for OpenAI's chat endpoint based on the model's history, system prompt, and input message\n",
    "6. The message is sent to OpenAI's chat endpoint, along with other parameters set during model initialization (such as temperature and max tokens)\n",
    "7. Finally, we parse return a list of responses from OpenAI's chat endpoint\n",
    "\n",
    "While there are quite a few steps here, note that the only ones specific to the `OpenAIChat` class are 5-8. \n",
    "\n",
    "***That means if you're creating a new custom class, all you need to worry about is getting a client instance, passing a message to the client, and returning its response.*** \n",
    "\n",
    "Everything else can be abstracted away via the model superclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### model/llm/openai/_openai.py  ###\n",
    "####################################\n",
    "from typing import Any\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "from artkit.model.util import RateLimitException\n",
    "from artkit.model.llm.history import ChatHistory\n",
    "\n",
    "from openai import RateLimitError\n",
    "\n",
    "\n",
    "async def get_response(\n",
    "    self,\n",
    "    message: str,\n",
    "    *,\n",
    "    history: ChatHistory | None = None,\n",
    "    **model_params: dict[str, Any],\n",
    ") -> list[str]:\n",
    "\n",
    "    # ARTKIT model implementations are designed to be fully asynchronous -\n",
    "    #   AsyncExitStack is used to handle multiple context managers dynamically.\n",
    "    async with AsyncExitStack():\n",
    "        try:\n",
    "            # We access the client instance via the get_client method of the \"ConnectorMixin\" superclass - \n",
    "            #   this will fetch a cached client instance if it exists or make a new one if it does not\n",
    "            # This is very helpful, as it means you can share the same client instance across model objects\n",
    "            completion = await self.get_client().chat.completions.create(\n",
    "\n",
    "                # Here is the only OpenAI specific bit of code - we're formatting the message\n",
    "                #  to pass to the chat endpoint\n",
    "                messages=list(\n",
    "                    self._messages_to_openai_format(  # type: ignore[arg-type]\n",
    "                        message, history=history\n",
    "                    )\n",
    "                ),\n",
    "                model=self.model_id,\n",
    "\n",
    "                # We merge the model parameters passed to the get_response method with the defaults set\n",
    "                # during instantiation, by overwriting the defaults with the passed parameters\n",
    "                **{**self.get_model_params(), **model_params},\n",
    "                )\n",
    "        except RateLimitError as e:\n",
    "                # If the rate limit is exceeded, we raise a custom RateLimitException\n",
    "                # This is caught for all ChatModelConnectors and handled via exponential backoff\n",
    "                raise RateLimitException(\n",
    "                    \"Rate limit exceeded. Please try again later.\"\n",
    "                ) from e\n",
    "\n",
    "    return list(self._responses_from_completion(completion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the actual output of our `.get_response()` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue as the endless sea,\n",
      "Reflecting the sun's bright glow,\n",
      "Infinite and free.\n"
     ]
    }
   ],
   "source": [
    "print((await gpt4_chat.with_system_prompt(\"You respond only in haiku\").get_response(message=\"What color is the sky?\"))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your implementation - a checklist:\n",
    "\n",
    "Here are the basic steps you'll need to take to create a new custom model class:\n",
    "\n",
    "1. Depending on which kind of model you want to implement the right abstract class e.g., `ChatModelConnect`. Ideally, your IDE assists you here. Otherwise, you can try starting with an existing implementation, but make sure to check your parent classes and method signatures.\n",
    "2. Update `__init__` to only include the parameters relevant for your model\n",
    "3. Update `_make_client` to return an instance of your model's client. To do so, review the model provider's API documentation; refer to [Connecting to Gen AI Models](../introduction_to_artkit/connecting_to_genai_models.ipynb) for some examples of what you're looking for\n",
    "4. Update `get_response` to pass a message to the client endpoint and return its response\n",
    "5. Add unit tests for your new model implementation\n",
    "\n",
    "If you're working with a diffusion or vision model, you will have to implement a different abstract model class but the necessary steps are very similar.\n",
    "\n",
    "Here are a few other best-practices that will save you time during development:\n",
    "\n",
    "- Run pre-commit hooks frequently; they will help you catch any missing implementation, type errors, or general formatting inconsistencies\n",
    "- Write unit tests as you go, and run `pytest` intermittently to make sure you haven't accidentally broken anything\n",
    "- Import your model class in `api.py``; this will allow it to be called via the ARTKIT API\n",
    "- Add a try / expect `ImportError` at the top of your class; ARTKIT does not require every supported model to be installed on setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
